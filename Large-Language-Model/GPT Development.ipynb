{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8dabb83c-5b5c-4c8e-9924-03127dc1cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6383f678-7445-4856-966e-0cecdd887b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset:  3639580\n",
      "First 1000 characters: \n",
      "\n",
      "THE MAHABHARATA\n",
      "\n",
      "BOOK ONE\n",
      "\n",
      "SECTION I\n",
      "Om! Having bowed down to Narayana and Nara, the most exalted male being, and also to the goddess Saraswati, must the word Jaya be uttered.\n",
      "\n",
      "Ugrasrava, the son of Lomaharshana, surnamed Sauti, well-versed in the Puranas, bending with humility, one day approached the great sages of rigid vows, sitting at their ease, who had attended the twelve years’ sacrifice of Saunaka, surnamed Kulapati, in the forest of Naimisha. Those ascetics, wishing to hear his wonderful narrations, presently began to address him who had thus arrived at that recluse abode of the inhabitants of the forest of Naimisha. Having been entertained with due respect by those holy men, he saluted those Munis (sages) with joined palms, even all of them, and inquired about the progress of their asceticism. Then all the ascetics being again seated, the son of Lomaharshana humbly occupied the seat that was assigned to him. Seeing that he was comfortably seated, and recovered from fatigue, o\n"
     ]
    }
   ],
   "source": [
    "with open(\"./mahabharata.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"Length of dataset: \", len(text))\n",
    "print(\"First 1000 characters: \\n\\n\"+text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "686f6db4-2b21-4fb8-b6a9-a181a8e7e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !&(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz—‘’“”\n",
      "Length: 81\n"
     ]
    }
   ],
   "source": [
    "# Get the set of all characters used \n",
    "chars = sorted(list(set(text)))\n",
    "print(\"\".join(chars))\n",
    "print(\"Length:\",len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed840d30-dbf3-4191-b2ce-13a5c3b2f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 50, 62, 50]\n",
      "Rama\n"
     ]
    }
   ],
   "source": [
    "# Convert each character into integer map -- tokenizer \n",
    "stoi = { char:int for int, char in enumerate(chars) }\n",
    "itos = { int:char for int, char in enumerate(chars) }\n",
    "encode = lambda word : [ stoi[char] for char in word ]\n",
    "decode = lambda arr : \"\".join([ itos[i] for i in arr ])\n",
    "\n",
    "print(encode(\"Rama\"))\n",
    "print(decode(encode(\"Rama\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9697b571-204e-4c10-b3ec-f93c1b1bb4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3639580]) torch.int64\n",
      "tensor([41, 29, 26,  1, 34, 22, 29, 22, 23, 29, 22, 39, 22, 41, 22,  0,  0, 23,\n",
      "        36, 36, 32,  1, 36, 35, 26,  0,  0, 40, 26, 24, 41, 30, 36, 35,  1, 30,\n",
      "         0, 36, 62,  2,  1, 29, 50, 71, 58, 63, 56,  1, 51, 64, 72, 54, 53,  1,\n",
      "        53, 64, 72, 63,  1, 69, 64,  1, 35, 50, 67, 50, 74, 50, 63, 50,  1, 50,\n",
      "        63, 53,  1, 35, 50, 67, 50,  6,  1, 69, 57, 54,  1, 62, 64, 68, 69,  1,\n",
      "        54, 73, 50, 61, 69, 54, 53,  1, 62, 50, 61, 54,  1, 51, 54, 58, 63, 56,\n",
      "         6,  1, 50, 63, 53,  1, 50, 61, 68, 64,  1, 69, 64,  1, 69, 57, 54,  1,\n",
      "        56, 64, 53, 53, 54, 68, 68,  1, 40, 50, 67, 50, 68, 72, 50, 69, 58,  6,\n",
      "         1, 62, 70, 68, 69,  1, 69, 57, 54,  1, 72, 64, 67, 53,  1, 31, 50, 74,\n",
      "        50,  1, 51, 54,  1, 70, 69, 69, 54, 67, 54, 53,  8,  0,  0, 42, 56, 67,\n",
      "        50, 68, 67, 50, 71, 50,  6,  1, 69, 57, 54,  1, 68, 64, 63,  1, 64, 55,\n",
      "         1, 33, 64, 62, 50, 57, 50, 67, 68, 57, 50, 63, 50,  6,  1, 68, 70, 67,\n",
      "        63, 50, 62, 54, 53,  1, 40, 50, 70, 69, 58,  6,  1, 72, 54, 61, 61,  7,\n",
      "        71, 54, 67, 68, 54, 53,  1, 58, 63,  1, 69, 57, 54,  1, 37, 70, 67, 50,\n",
      "        63, 50, 68,  6,  1, 51, 54, 63, 53, 58, 63, 56,  1, 72, 58, 69, 57,  1,\n",
      "        57, 70, 62, 58, 61, 58, 69, 74,  6,  1, 64, 63, 54,  1, 53, 50, 74,  1,\n",
      "        50, 65, 65, 67, 64, 50, 52, 57, 54, 53,  1, 69, 57, 54,  1, 56, 67, 54,\n",
      "        50, 69,  1, 68, 50, 56, 54, 68,  1, 64, 55,  1, 67, 58, 56, 58, 53,  1,\n",
      "        71, 64, 72, 68,  6,  1, 68, 58, 69, 69, 58, 63, 56,  1, 50, 69,  1, 69,\n",
      "        57, 54, 58, 67,  1, 54, 50, 68, 54,  6,  1, 72, 57, 64,  1, 57, 50, 53,\n",
      "         1, 50, 69, 69, 54, 63, 53, 54, 53,  1, 69, 57, 54,  1, 69, 72, 54, 61,\n",
      "        71, 54,  1, 74, 54, 50, 67, 68, 78,  1, 68, 50, 52, 67, 58, 55, 58, 52,\n",
      "        54,  1, 64, 55,  1, 40, 50, 70, 63, 50, 60, 50,  6,  1, 68, 70, 67, 63,\n",
      "        50, 62, 54, 53,  1, 32, 70, 61, 50, 65, 50, 69, 58,  6,  1, 58, 63,  1,\n",
      "        69, 57, 54,  1, 55, 64, 67, 54, 68, 69,  1, 64, 55,  1, 35, 50, 58, 62,\n",
      "        58, 68, 57, 50,  8,  1, 41, 57, 64, 68, 54,  1, 50, 68, 52, 54, 69, 58,\n",
      "        52, 68,  6,  1, 72, 58, 68, 57, 58, 63, 56,  1, 69, 64,  1, 57, 54, 50,\n",
      "        67,  1, 57, 58, 68,  1, 72, 64, 63, 53, 54, 67, 55, 70, 61,  1, 63, 50,\n",
      "        67, 67, 50, 69, 58, 64, 63, 68,  6,  1, 65, 67, 54, 68, 54, 63, 69, 61,\n",
      "        74,  1, 51, 54, 56, 50, 63,  1, 69, 64,  1, 50, 53, 53, 67, 54, 68, 68,\n",
      "         1, 57, 58, 62,  1, 72, 57, 64,  1, 57, 50, 53,  1, 69, 57, 70, 68,  1,\n",
      "        50, 67, 67, 58, 71, 54, 53,  1, 50, 69,  1, 69, 57, 50, 69,  1, 67, 54,\n",
      "        52, 61, 70, 68, 54,  1, 50, 51, 64, 53, 54,  1, 64, 55,  1, 69, 57, 54,\n",
      "         1, 58, 63, 57, 50, 51, 58, 69, 50, 63, 69, 68,  1, 64, 55,  1, 69, 57,\n",
      "        54,  1, 55, 64, 67, 54, 68, 69,  1, 64, 55,  1, 35, 50, 58, 62, 58, 68,\n",
      "        57, 50,  8,  1, 29, 50, 71, 58, 63, 56,  1, 51, 54, 54, 63,  1, 54, 63,\n",
      "        69, 54, 67, 69, 50, 58, 63, 54, 53,  1, 72, 58, 69, 57,  1, 53, 70, 54,\n",
      "         1, 67, 54, 68, 65, 54, 52, 69,  1, 51, 74,  1, 69, 57, 64, 68, 54,  1,\n",
      "        57, 64, 61, 74,  1, 62, 54, 63,  6,  1, 57, 54,  1, 68, 50, 61, 70, 69,\n",
      "        54, 53,  1, 69, 57, 64, 68, 54,  1, 34, 70, 63, 58, 68,  1,  4, 68, 50,\n",
      "        56, 54, 68,  5,  1, 72, 58, 69, 57,  1, 59, 64, 58, 63, 54, 53,  1, 65,\n",
      "        50, 61, 62, 68,  6,  1, 54, 71, 54, 63,  1, 50, 61, 61,  1, 64, 55,  1,\n",
      "        69, 57, 54, 62,  6,  1, 50, 63, 53,  1, 58, 63, 66, 70, 58, 67, 54, 53,\n",
      "         1, 50, 51, 64, 70, 69,  1, 69, 57, 54,  1, 65, 67, 64, 56, 67, 54, 68,\n",
      "        68,  1, 64, 55,  1, 69, 57, 54, 58, 67,  1, 50, 68, 52, 54, 69, 58, 52,\n",
      "        58, 68, 62,  8,  1, 41, 57, 54, 63,  1, 50, 61, 61,  1, 69, 57, 54,  1,\n",
      "        50, 68, 52, 54, 69, 58, 52, 68,  1, 51, 54, 58, 63, 56,  1, 50, 56, 50,\n",
      "        58, 63,  1, 68, 54, 50, 69, 54, 53,  6,  1, 69, 57, 54,  1, 68, 64, 63,\n",
      "         1, 64, 55,  1, 33, 64, 62, 50, 57, 50, 67, 68, 57, 50, 63, 50,  1, 57,\n",
      "        70, 62, 51, 61, 74,  1, 64, 52, 52, 70, 65, 58, 54, 53,  1, 69, 57, 54,\n",
      "         1, 68, 54, 50, 69,  1, 69, 57, 50, 69,  1, 72, 50, 68,  1, 50, 68, 68,\n",
      "        58, 56, 63, 54, 53,  1, 69, 64,  1, 57, 58, 62,  8,  1, 40, 54, 54, 58,\n",
      "        63, 56,  1, 69, 57, 50, 69,  1, 57, 54,  1, 72, 50, 68,  1, 52, 64, 62,\n",
      "        55, 64, 67, 69, 50, 51, 61, 74,  1, 68, 54, 50, 69, 54, 53,  6,  1, 50,\n",
      "        63, 53,  1, 67, 54, 52, 64, 71, 54, 67, 54, 53,  1, 55, 67, 64, 62,  1,\n",
      "        55, 50, 69, 58, 56, 70, 54,  6,  1, 64])\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset into integer map\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71f3fed4-5794-4115-80a0-242ae8556bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first 90% to train, rest will be validation\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fdba299a-b875-4a67-84dd-474ef5b7f665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41, 29, 26,  1, 34, 22, 29, 22, 23])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum length of 'snip' of data being trained \n",
    "block_size = 8 \n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db460f02-7bcf-4e01-9ffb-1a0522a49e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([41]) the target is: 29\n",
      "When the input is tensor([41, 29]) the target is: 26\n",
      "When the input is tensor([41, 29, 26]) the target is: 1\n",
      "When the input is tensor([41, 29, 26,  1]) the target is: 34\n",
      "When the input is tensor([41, 29, 26,  1, 34]) the target is: 22\n",
      "When the input is tensor([41, 29, 26,  1, 34, 22]) the target is: 29\n",
      "When the input is tensor([41, 29, 26,  1, 34, 22, 29]) the target is: 22\n",
      "When the input is tensor([41, 29, 26,  1, 34, 22, 29, 22]) the target is: 23\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When the input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ac55b1a-a7a6-4a18-b20c-63ce5e426fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[50, 58, 68, 50, 62, 65, 50, 74],\n",
      "        [58, 69, 68,  6,  1, 50, 68,  1],\n",
      "        [57, 50,  6,  1, 69, 57, 64, 70],\n",
      "        [ 1, 58, 63, 68, 69, 50, 61, 61]])\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 68, 50, 62, 65, 50, 74, 50],\n",
      "        [69, 68,  6,  1, 50, 68,  1, 50],\n",
      "        [50,  6,  1, 69, 57, 64, 70,  1],\n",
      "        [58, 63, 68, 69, 50, 61, 61, 54]])\n",
      "----\n",
      "when input is [50] the target: 58\n",
      "when input is [50, 58] the target: 68\n",
      "when input is [50, 58, 68] the target: 50\n",
      "when input is [50, 58, 68, 50] the target: 62\n",
      "when input is [50, 58, 68, 50, 62] the target: 65\n",
      "when input is [50, 58, 68, 50, 62, 65] the target: 50\n",
      "when input is [50, 58, 68, 50, 62, 65, 50] the target: 74\n",
      "when input is [50, 58, 68, 50, 62, 65, 50, 74] the target: 50\n",
      "when input is [58] the target: 69\n",
      "when input is [58, 69] the target: 68\n",
      "when input is [58, 69, 68] the target: 6\n",
      "when input is [58, 69, 68, 6] the target: 1\n",
      "when input is [58, 69, 68, 6, 1] the target: 50\n",
      "when input is [58, 69, 68, 6, 1, 50] the target: 68\n",
      "when input is [58, 69, 68, 6, 1, 50, 68] the target: 1\n",
      "when input is [58, 69, 68, 6, 1, 50, 68, 1] the target: 50\n",
      "when input is [57] the target: 50\n",
      "when input is [57, 50] the target: 6\n",
      "when input is [57, 50, 6] the target: 1\n",
      "when input is [57, 50, 6, 1] the target: 69\n",
      "when input is [57, 50, 6, 1, 69] the target: 57\n",
      "when input is [57, 50, 6, 1, 69, 57] the target: 64\n",
      "when input is [57, 50, 6, 1, 69, 57, 64] the target: 70\n",
      "when input is [57, 50, 6, 1, 69, 57, 64, 70] the target: 1\n",
      "when input is [1] the target: 58\n",
      "when input is [1, 58] the target: 63\n",
      "when input is [1, 58, 63] the target: 68\n",
      "when input is [1, 58, 63, 68] the target: 69\n",
      "when input is [1, 58, 63, 68, 69] the target: 50\n",
      "when input is [1, 58, 63, 68, 69, 50] the target: 61\n",
      "when input is [1, 58, 63, 68, 69, 50, 61] the target: 61\n",
      "when input is [1, 58, 63, 68, 69, 50, 61, 61] the target: 54\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # The number of independent sequences processing in parallel \n",
    "block_size = 8 # The maximum context length for predictions\n",
    " \n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch. randint(len (data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb, = get_batch(\"train\")\n",
    "print(\"Inputs: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets: \")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a2927db5-d339-46fc-88eb-c0ca842a3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 81])\n",
      "tensor(4.5380, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "!KMf?&&Bo—p)]l SWqc3kFZ—ixdKvsRJs’—F)—‘“xbae5Q;;Zj?rZ;ripf1H0k!—6Ky9—L“baOlqB\n",
      "L:]4WP iuB63or]!s[IhRr\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(len(chars))\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbda9d8-c1af-40de-abd7-a3889d293d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
